---
title: 'Designing and Implementing a Connected Health Program - Lessons Learned'
date: '2019-04-23'
summary: >-
  From ideation through implementation, the Way to Health team has a lot of
  experience running connected health programs and research trials. Hear from
  the people behind the curtain about the best ways to deploy effective programs
  at scale. 
image: /images/bg/connected-health.svg
authorname: Christianne Sevinc
authorimage: /images/team/sevinc.jpg
label: Project Design
---
The Way To Health team works with a variety of investigators – behavioral economists, clinicians, MDs, PhDs, RNs, etc. While the investigators have varied experiences and expertise, they all have one thing in common, they’ve got an idea. To be precise, they have a hypothesis rooted in theory and empirical evidence, either in the same domain or drawn from another domain. They are trying to figure out ways to keep people healthier, change behaviors for good, sometimes with a dose of savings for the health system, health plans or wellness in general.

Examples of questions and hypotheses include:

- “How do I get my employees to be more physically active, and those with high BMIs to lose weight?”

- “How can I get more of my patients to properly prepare for a colonoscopy? And more importantly, how can I ensure that they even show up?” This question led to the design of a pilot study and its success had led to the design and current deployment of a  broader randomized control trial: The [ColoPrep pilot study](https://www.waytohealth.org/casestudies/coloprep/). The pilot study showed by simply texting patients prior to their scheduled colonoscopy, they could reduce no show rates by 100% (current no show rates at Penn Medicine are around 40%).

- “Will the application of gamification with enhanced social supports improve activity levels among patients with Parkinson’s? And subsequently can increased physical activity reduce apathy, a common symptom in this population? The BE FIT trial showed substantial increase in physical activity among the general population. Can this be applied and be effective in different populations? There are ongoing studies to test this hypothesis. 

These seemingly simple questions and hypotheses are actually quite complex because humans don’t always act in our own best interest, especially in regard to our health. Luckily technology provides a way to interact and connect with people in their daily lives to keep them on track.

So, the investigator has an idea that goes something like this: _“I want to test the effectiveness of \[insert behavioral intervention strategy] to \[insert desired behavior] among \[insert target population]."_ Let’s try one on for size. _“I want to test the effectiveness of loss-framed financial incentives to increase medication adherence among patients with high cholesterol”._ They write out a high-level strategy for tackling the problem and are ready to hit the ground running. But someone needs to translate that high-level plan into a detailed day-by-day protocol. This is where the Way to Health team comes in.

The Way To Health team has run a lot of these trials, 116 to be precise. We like to think of ourselves as an implementation focused behavioral economists by trade. As such, here are some of our learnings:

## Walk a mile in someone else's shoes

We encourage our teams to pretend to be the participant. 

- Walk through the enrollment process, whether it be in clinic, or completed remotely by sending a link over email to participate. 
- Next, try the intervention on for size. Are you annoyed by the frequency of the daily texts? Was the content of the text message appropriate for the age group? 
- Device connection: Was figuring out how to setup the Fitbit challenging? How do you feel about wearing the Fitbit all the time, even when you sleep? Did you sync your device, or miss and get upset that you didn’t reach your goal? 

Depending on your population, you might have to stretch your imagination. As an example, while I might be annoyed by five texts a day, an isolated elderly person may welcome the chance to engage with the program. On the opposite end of the spectrum, a teenager who receives hundreds of texts a day might need something extra (emojis anyone?) to catch their attention. 

## Keep it simple

When investigators find out [how much Way to Health can do](https://www.waytohealth.org/platform), they start spinning with ideas. 

- *"What if I layer gamification with financial incentives, and have my participants connected to a scale, pedometer, and blood pressure cuff, then change their goals on a weekly basis based on last week’s data?"*

I jokingly call these Long Island iced tea or “LIIT” (rhymes with leet) projects. There is just way too much going on which means that not only will building and maintaining the project be a challenge but more importantly, the patient or the participant may not understand what they’re doing or need to be doing and why. For example, [a recent article](https://jamanetwork.com/journals/jama/fullarticle/2729548) talks about why financial incentives work in some cases and don’t in others primarily because of a lack of participant understanding. An important lesson is that seemingly small choices in design can have substantial effects on their success. We often guide researchers to pull out the most meaningful component to their intervention and build a simpler protocol that can be deployed and managed with ease while still achieving the desired behavior change or health goal.

## Fake it till you make it

Do a pilot or test run with partial or no automation. We call this implementing a “fake backend”. This means you are the woman behind the curtain or the mechanical turk, trying to figure out what works before trying to automate it. This could be testing out messaging with someone sitting behind the scenes answering as if he or she was the computer, or it could be manually calculating adherence rates, or manually tracking biometric device data before a full device integration. We have found this approach to be very useful primarily because it often helps invalidate some initial assumptions. 

Additionally, these pilots can be very rapid and much cheaper to run. Once you shown some success with such a fake backend pilot, you are ready to talk automation. You will have identified pitfalls and potential changes in the protocol based on direct feedback from the end users. Thus, the project design will be validated and much easier to scale.

Hopefully this post provokes some thought on study design. Please feel free to drop us a note with comments and / or questions. 
